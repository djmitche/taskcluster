version: 20
migrationScript: 0020-migration.sql
downgradeScript: 0020-downgrade.sql
methods:

  ####
  # queue_tasks_entities

  queue_tasks_entities_load:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: partition_key text, row_key text
    returns: table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)
    body: |-
      begin
        -- get the task from the new table, falling back to the old
        perform 1 from tasks where tasks.task_id = partition_key;
        if found then
          -- new table (from original version 0020)
          return query
          select
            queue_tasks_entities_load.partition_key,
            queue_tasks_entities_load.row_key,
            entity_buf_encode(
              entity_buf_encode(
                entity_buf_encode(
                  entity_buf_encode(
                    entity_buf_encode(
                      entity_buf_encode(
                        entity_buf_encode(
                          entity_buf_encode(
                            jsonb_build_object(
                              'PartitionKey', task_id,
                              'RowKey', 'task',
                              'taskId', slugid_to_uuid(task_id),
                              'provisionerId', provisioner_id,
                              'workerType', worker_type,
                              'schedulerId', scheduler_id,
                              'taskGroupId', slugid_to_uuid(task_group_id),
                              'requires', requires::text,
                              'priority', priority::text,
                              'retries', retries,
                              'retriesLeft', retries_left,
                              'created', created,
                              'deadline', deadline,
                              'expires', expires,
                              'takenUntil', coalesce(taken_until, '1970-01-01 00:00:00+00'::timestamptz)),
                            'dependencies', dependencies::text),
                          'routes', routes::text),
                        'scopes', scopes::text),
                      'payload', payload::text),
                    'metadata', metadata::text),
                  'tags', tags::text),
                'extra', extra::text),
              'runs', runs::text) as value,
            1 as version,
            tasks.etag as etag
          from tasks
          where
            tasks.task_id = partition_key;
        else
          -- old table (copied from version 0003)
          return query
          select queue_tasks_entities.partition_key, queue_tasks_entities.row_key, queue_tasks_entities.value, queue_tasks_entities.version,
          queue_tasks_entities.etag from queue_tasks_entities
          where queue_tasks_entities.partition_key = queue_tasks_entities_load.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_load.row_key;
        end if;
      end
  queue_tasks_entities_create:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: pk text, rk text, properties jsonb, overwrite boolean, version integer
    returns: uuid
    body: |-
      declare
        new_row tasks%ROWTYPE;
      begin
        -- create the task in the new table, checking for conflicts in the old
        perform 1 from queue_tasks_entities where queue_tasks_entities.partition_key = pk;
        if found then
          raise exception 'task exists in queue_tasks_entities' using errcode = 'unique_violation';
        end if;
        -- remainder is from original 0020.yml
        select
          uuid_to_slugid(properties ->> 'taskId'),
          (properties ->> 'provisionerId')::text,
          (properties ->> 'workerType')::text,
          (properties ->> 'schedulerId')::text,
          uuid_to_slugid(properties ->> 'taskGroupId')::text,
          entity_buf_decode(properties, 'dependencies')::jsonb,
          (properties ->> 'requires')::task_requires,
          entity_buf_decode(properties, 'routes')::jsonb,
          (properties ->> 'priority')::task_priority,
          (properties ->> 'retries')::int,
          (properties ->> 'retriesLeft')::int,
          (properties ->> 'created')::timestamptz,
          (properties ->> 'deadline')::timestamptz,
          (properties ->> 'expires')::timestamptz,
          entity_buf_decode(properties, 'scopes')::jsonb,
          entity_buf_decode(properties, 'payload')::jsonb,
          entity_buf_decode(properties, 'metadata')::jsonb,
          entity_buf_decode(properties, 'tags')::jsonb,
          entity_buf_decode(properties, 'extra')::jsonb,
          entity_buf_decode(properties, 'runs')::jsonb,
          case (properties ->> 'takenUntil')::timestamptz
            when '1970-01-01 00:00:00+00'::timestamptz then null
            else (properties ->> 'takenUntil')::timestamptz
          end,
          false,
          public.gen_random_uuid()
        into new_row;
        if overwrite then
          raise exception 'overwrite not implemented';
        else
          execute 'insert into tasks select $1.*' using new_row;
        end if;
        return new_row.etag;
      end
  queue_tasks_entities_remove:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text
    returns: table (etag uuid)
    body: |-
      begin
        -- queue removes tasks for two reasons: during expiration, and when a task is
        -- partially created before detecting an error (such as missing dependencies).
        -- So, during this migration, only new tasks can be removed.
        return query delete from tasks
        where
          tasks.task_id = partition_key
        returning tasks.etag;
      end
  queue_tasks_entities_modify:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text, properties jsonb, version integer, old_etag uuid
    returns: table (etag uuid)
    body: |-
      declare
        new_etag uuid := public.gen_random_uuid();
      begin
        perform 1 from tasks where tasks.task_id = partition_key;
        if found then
          -- from original 0020
          -- NOTE: queue only updates runs, retriesLeft, and takenUntil, so only those fields are
          -- supported here.
          update tasks
          set (
            runs,
            retries_left,
            taken_until,
            etag
          ) = (
            entity_buf_decode(properties, 'runs')::jsonb,
            (properties ->> 'retriesLeft')::int,
            case (properties ->> 'takenUntil')::timestamptz
              when '1970-01-01 00:00:00+00'::timestamptz then null
              else (properties ->> 'takenUntil')::timestamptz
            end,
            new_etag
          )
          where
            tasks.task_id = partition_key and
            tasks.etag = queue_tasks_entities_modify.old_etag;
          if found then
            return query select new_etag;
            return;
          end if;
          perform tasks.etag from tasks
          where
            tasks.task_id = partition_key;
          if found then
            raise exception 'unsuccessful update' using errcode = 'P0004';
          else
            raise exception 'no such row' using errcode = 'P0002';
          end if;
        else
          -- from 0002
          update queue_tasks_entities
          set (value, version, etag) = (properties, queue_tasks_entities_modify.version, new_etag)
          where queue_tasks_entities.partition_key = queue_tasks_entities_modify.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_modify.row_key and queue_tasks_entities.etag = queue_tasks_entities_modify.old_etag;

          if found then
            return query select new_etag;
            return;
          end if;

          perform queue_tasks_entities.etag from queue_tasks_entities
          where queue_tasks_entities.partition_key = queue_tasks_entities_modify.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_modify.row_key;

          if found then
            raise exception 'unsuccessful update' using errcode = 'P0004';
          else
            raise exception 'no such row' using errcode = 'P0002';
          end if;
        end if;
      end
  queue_tasks_entities_scan:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: pk text, rk text, condition text, size integer, page integer
    returns: table (partition_key text, row_key text, value jsonb, version integer, etag uuid)
    body: |-
      declare
        cond text[];
        exp_cond_field text;
        exp_cond_operator text;
        exp_cond_operand timestamptz;
        sql text := 'select queue_tasks_entities.partition_key, queue_tasks_entities.row_key, queue_tasks_entities.value, queue_tasks_entities.version, queue_tasks_entities.etag from queue_tasks_entities';
        partition_key_var text;
        row_key_var text;
      begin
        if not condition is null then
          cond := regexp_split_to_array(condition, '\s+');
          exp_cond_field := trim(cond[3], '''');
          exp_cond_operator := cond[4];
          exp_cond_operand := cond[5] :: timestamptz;

          if (exp_cond_field || exp_cond_operator) = 'expires<' then
            -- during the migration, nothing gets expired..
            return;
          end if;

          if not (exp_cond_field || exp_cond_operator) in ('takenUntil=', 'deadline=', 'expires<') then
            raise exception 'queue_tasks_entities_scan only supports certain takenUntil, deadline, and expires conditions';
          end if;
        end if;

        -- pk is set in all circumstances except expires, which is handled above
        if pk is null then
          raise exception 'non-expiration task entities scan with null pk?!';
        end if;

        perform 1
        from tasks
        where tasks.task_id = pk;

        if found then
          -- query from original 0020
          return query select
            task_id as partition_key,
            'task' as row_key,
            entity_buf_encode(
              entity_buf_encode(
                entity_buf_encode(
                  entity_buf_encode(
                    entity_buf_encode(
                      entity_buf_encode(
                        entity_buf_encode(
                          entity_buf_encode(
                            jsonb_build_object(
                              'PartitionKey', task_id,
                              'RowKey', 'task',
                              'taskId', slugid_to_uuid(task_id),
                              'provisionerId', provisioner_id,
                              'workerType', worker_type,
                              'schedulerId', scheduler_id,
                              'taskGroupId', slugid_to_uuid(task_group_id),
                              'requires', requires::text,
                              'priority', priority::text,
                              'retries', retries,
                              'retriesLeft', retries_left,
                              'created', created,
                              'deadline', deadline,
                              'expires', expires,
                              'takenUntil', coalesce(taken_until, '1970-01-01 00:00:00+00'::timestamptz)),
                            'dependencies', dependencies::text),
                          'routes', routes::text),
                        'scopes', scopes::text),
                      'payload', payload::text),
                    'metadata', metadata::text),
                  'tags', tags::text),
                'extra', extra::text),
              'runs', runs::text) as value,
            1 as version,
            tasks.etag as etag from tasks
          where
            (pk is NULL or pk = task_id) and
            case
              when exp_cond_field = 'deadline' then deadline = exp_cond_operand
              -- note that queue never queries for takenUntil = new Date(0)
              when exp_cond_field = 'takenUntil' then taken_until = exp_cond_operand
              when exp_cond_field = 'expires' then expires < exp_cond_operand
              else true
            end
          order by tasks.task_id
          limit case
            when (size is not null and size > 0) then size + 1
            else null
          end
          offset case
            when (page is not null and page > 0) then page
            else 0
          end;
        else
          -- entities scan from 0007
          if queue_tasks_entities_scan.pk is not null or queue_tasks_entities_scan.rk is not null or condition is not null then
            sql := sql || ' where ';
          end if;

          if queue_tasks_entities_scan.pk is not null then
            partition_key_var := 'partition_key = ' || quote_literal(queue_tasks_entities_scan.pk);
          end if;

          if queue_tasks_entities_scan.rk is not null then
            row_key_var := 'row_key = ' || quote_literal(queue_tasks_entities_scan.rk);
          end if;

          sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);
          sql := sql || ' order by queue_tasks_entities.partition_key, queue_tasks_entities.row_key';

          if size is not null and size > 0 then
            sql := sql || ' limit ' || size + 1;

            if page is not null and page > 0 then
              sql := sql || ' offset ' || page;
            end if;
          end if;

          return query execute sql;
        end if;
      end

  ####
  # queue_task_group_members_entities

  queue_task_group_members_entities_load:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: partition_key text, row_key text
    returns: table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)
    body: |-
      begin
        perform 1
        from tasks
        where
          tasks.task_group_id = partition_key and
          tasks.task_id = row_key;
        if found then
          -- query from original 0020
          return query
          select
            task_group_id,
            task_id,
            jsonb_build_object(
              'PartitionKey', task_group_id,
              'RowKey', task_id,
              'taskId', slugid_to_uuid(task_id),
              'taskGroupId', slugid_to_uuid(task_group_id),
              'expires', expires) as value,
            1 as version,
            tasks.etag as etag
          from tasks
          where
            tasks.task_group_id = partition_key and
            tasks.task_id = row_key;
        else
          -- query from 0002
          return query
          select queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key, queue_task_group_members_entities.value, queue_task_group_members_entities.version,
          queue_task_group_members_entities.etag from queue_task_group_members_entities
          where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_load.partition_key and queue_task_group_members_entities.row_key = queue_task_group_members_entities_load.row_key;
        end if;
      end
  queue_task_group_members_entities_create:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: pk text, rk text, properties jsonb, overwrite boolean, version integer
    returns: uuid
    body: |-
      begin
        -- during the migration, new tasks are created in the tasks table, which implies
        -- group membership, so there's nothing to do here.
        return public.gen_random_uuid();
      end
  queue_task_group_members_entities_remove:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text
    returns: table (etag uuid)
    body: |-
      begin
        -- during this migration, removing tasks (and their members) does nothing
        return query select public.gen_random_uuid() as etag;
      end
  queue_task_group_members_entities_modify:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text, properties jsonb, version integer, old_etag uuid
    returns: table (etag uuid)
    body: |-
      begin
        raise exception 'modify not implemented for queue_task_group_members_entities';
      end
  queue_task_group_members_entities_scan:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: pk text, rk text, condition text, size integer, page integer
    returns: table (partition_key text, row_key text, value jsonb, version integer, etag uuid)
    body: |-
      declare
        cond text[];
        exp_cond_field text;
        exp_cond_operator text;
        exp_cond_operand timestamptz;
      begin
        if pk is null then
          -- if pk is null, then this is the expiration scan for the members table; since
          -- there is no members table, we stub this out by simply returning an empty set,
          -- ignoring the condition
          return;
        end if;

        raise log 'here size % page %', size, page;

        if not condition is null then
          cond := regexp_split_to_array(condition, '\s+');
          exp_cond_field := trim(cond[3], '''');
          exp_cond_operator := cond[4];
          exp_cond_operand := cond[5] :: timestamptz;

          if exp_cond_operator != '>=' or exp_cond_field != 'expires' then
            raise exception 'queue_task_group_memberss_entities_scan only supports `expires >= <timestamp>` conditions';
          end if;
        end if;

        -- Here's the plan: construct a union of the query from 0007 and the query from 0020, since some
        -- of the tasks in this task group may be migrated and others not.  That union is 'results', and
        -- it is then sorted and paginated.
        return query with results as (

          -- query from 0007, rewritten to not use conditions or EXECUTE
          select
            queue_task_group_members_entities.partition_key,
            queue_task_group_members_entities.row_key,
            queue_task_group_members_entities.value,
            queue_task_group_members_entities.version,
            queue_task_group_members_entities.etag
          from queue_task_group_members_entities
          where 
            queue_task_group_members_entities.partition_key = pk
            -- expires condition is handled in the outer query

          union

          -- query from 0020
          select
            task_group_id,
            task_id,
            jsonb_build_object(
              'PartitionKey', task_group_id,
              'RowKey', task_id,
              'taskId', slugid_to_uuid(task_id),
              'taskGroupId', slugid_to_uuid(task_group_id),
              'expires', expires) as value,
            1 as version,
            tasks.etag as etag
          from tasks
          where
            tasks.task_group_id = pk
            -- expires condition is handled in the outer query
        )

        select * from results
        where exp_cond_operand is NULL or (results.value ->> 'expires')::timestamptz >= exp_cond_operand
        order by results.partition_key, results.row_key
        limit case
          when (size is not null and size > 0) then size + 1
          else null
        end
        offset case
          when (page is not null and page > 0) then page
          else 0
        end;
      end

  ####
  # queue_task_group_active_sets_entities

  queue_task_group_active_sets_entities_load:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: partition_key text, row_key text
    returns: table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)
    body: |-
      begin
        perform 1
        from tasks
        where
          tasks.task_group_id = partition_key and
          tasks.task_id = row_key;
        if found then
          -- query from original 0020
          return query
          select
            task_group_id,
            task_id,
            jsonb_build_object(
              'PartitionKey', task_group_id,
              'RowKey', task_id,
              'taskId', slugid_to_uuid(task_id),
              'taskGroupId', slugid_to_uuid(task_group_id),
              'expires', expires) as value,
            1 as version,
            tasks.etag as etag
          from tasks
          where
            not ever_resolved and
            tasks.task_group_id = partition_key and
            tasks.task_id = row_key;
        else
          -- query from 0002
          return query
          select queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key, queue_task_group_active_sets_entities.value, queue_task_group_active_sets_entities.version,
          queue_task_group_active_sets_entities.etag from queue_task_group_active_sets_entities
          where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_load.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_load.row_key;
        end if;
      end
  queue_task_group_active_sets_entities_create:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: pk text, rk text, properties jsonb, overwrite boolean, version integer
    returns: uuid
    body: |-
      begin
        -- this method is only called before the task is created, and tasks are
        -- created in an unresolved state, so there's nothing to do here.  Note
        -- that the service does not mark a task as unresolved when rerunning it
        return public.gen_random_uuid();
      end
  queue_task_group_active_sets_entities_remove:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text
    returns: table (etag uuid)
    body: |-
      begin
        perform 1
        from tasks
        where
          tasks.task_group_id = partition_key and
          tasks.task_id = row_key;
        if found then
          -- from 0020
          -- DependencyTracker uses this to mark a task as ever_resolved
          update tasks
          set ever_resolved = true
          where 
            tasks.task_group_id = partition_key and
            tasks.task_id = row_key;
          return query select public.gen_random_uuid() as etag;
        else
          -- from 0002
          return query delete from queue_task_group_active_sets_entities
          where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_remove.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_remove.row_key
          returning queue_task_group_active_sets_entities.etag;
        end if;
      end
  queue_task_group_active_sets_entities_modify:
    serviceName: queue
    description: See taskcluster-lib-entities
    mode: write
    args: partition_key text, row_key text, properties jsonb, version integer, old_etag uuid
    returns: table (etag uuid)
    body: |-
      begin
        raise exception 'modify not supported';
      end
  queue_task_group_active_sets_entities_scan:
    description: See taskcluster-lib-entities
    mode: read
    serviceName: queue
    args: pk text, rk text, condition text, size integer, page integer
    returns: table (partition_key text, row_key text, value jsonb, version integer, etag uuid)
    body: |-
      declare
        cond text[];
        exp_cond_field text;
        exp_cond_operator text;
        exp_cond_operand timestamptz;
      begin
        if not condition is null then
          cond := regexp_split_to_array(condition, '\s+');
          exp_cond_field := trim(cond[3], '''');

          if exp_cond_field != 'expires' then
            raise exception 'queue_task_group_active_sets_entities_scan only supports filtering for expired rows';
          end if;

          -- there's no distinct table to expire, so just return an empty set to make the
          -- expiration crontask finish immediately
          return;
        end if;

        -- Here's the plan: construct a union of the query from 0007 and the query from 0020, since some
        -- of the tasks in this task group may be migrated and others not.  That union is 'results', and
        -- it is then sorted and paginated.
        return query with results as (
          select
            queue_task_group_active_sets_entities.partition_key,
            queue_task_group_active_sets_entities.row_key,
            queue_task_group_active_sets_entities.value,
            queue_task_group_active_sets_entities.version,
            queue_task_group_active_sets_entities.etag
          from queue_task_group_active_sets_entities
          where 
            queue_task_group_active_sets_entities.partition_key = pk

          union

          -- query from 0020
          select
            task_group_id as partition_key,
            task_id as row_key,
            jsonb_build_object(
              'PartitionKey', task_group_id,
              'RowKey', task_id,
              'taskGroupId', slugid_to_uuid(task_group_id),
              'taskId', slugid_to_uuid(task_id),
              'expires', expires) as value,
            1 as version,
            public.gen_random_uuid() as etag
          from tasks
          where
            (pk is NULL or pk = task_group_id) and
            not ever_resolved
        )
        select * from results
        order by results.partition_key, results.row_key
        limit case
          when (size is not null and size > 0) then size + 1
          else null
        end
        offset case
          when (page is not null and page > 0) then page
          else 0
        end;
      end
